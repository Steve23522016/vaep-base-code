{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from socceraction.data.wyscout import PublicWyscoutLoader\n",
    "from socceraction.spadl.wyscout import convert_to_actions as convert_to_actions_wyscout\n",
    "from socceraction.spadl.statsbomb import convert_to_actions as convert_to_actions_statsbomb\n",
    "from socceraction.data.opta import OptaLoader\n",
    "from socceraction.data.statsbomb import StatsBombLoader\n",
    "from socceraction.spadl.config import actiontypes, bodyparts\n",
    "import socceraction.spadl as spadl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, brier_score_loss, log_loss, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, balanced_accuracy_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from rapidfuzz import fuzz\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_selection import r_regression, SelectKBest, chi2, mutual_info_classif, SequentialFeatureSelector, RFECV, SelectFromModel, mutual_info_regression, f_regression\n",
    "from scipy.stats import pearsonr, chisquare\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR, SVC\n",
    "from sklearn.linear_model import Lasso, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG FOR EXPERIMENTS SCENARIO\n",
    "USE_EVALUATION_METRIC_CLASSIFICATION = True\n",
    "SAMPLING_OPTIONS = [\n",
    "    \"none\",\n",
    "    \"random_oversampled\",\n",
    "    \"random_undersampled\",\n",
    "    \"smote_oversampled\"\n",
    "]\n",
    "FEATURE_SELECTION_OPTIONS = [\n",
    "    \"pearson\",\n",
    "    \"chisquare\",\n",
    "    \"mutualinf\",\n",
    "    \"mrmr\",\n",
    "    \"rfembedded\",\n",
    "    \"lasso\"\n",
    "]\n",
    "FEATURE_SELECTION_OPTIONS_FOR_PLAYER_PROBS = [\n",
    "    \"pearson\",\n",
    "    \"mutualinf\"\n",
    "]\n",
    "if USE_EVALUATION_METRIC_CLASSIFICATION:\n",
    "    MODEL_ALGORITHM_OPTIONS = [\n",
    "        \"xgbclassifier\",\n",
    "        # \"catboostclassifier\",\n",
    "        # \"rfclassifier\"\n",
    "    ]\n",
    "else:\n",
    "    MODEL_ALGORITHM_OPTIONS = [\n",
    "        \"xgbregressor\",\n",
    "        # \"rfregressor\",\n",
    "        # \"logregression\"\n",
    "    ]\n",
    "CONFIG_EXPERIMENTS_SCENARIO_MAP = {}\n",
    "\n",
    "# def construct_config_experiments_scenario_map():\n",
    "#     index_counter = 1\n",
    "#     for sampling_opt in SAMPLING_OPTIONS:\n",
    "#         for feature_selection_opt in FEATURE_SELECTION_OPTIONS:\n",
    "#             for algorithm_opt in MODEL_ALGORITHM_OPTIONS:\n",
    "#                 CONFIG_EXPERIMENTS_SCENARIO_MAP[index_counter] = \\\n",
    "#                     {\"sampling_opt\" : sampling_opt, \\\n",
    "#                     \"feature_selection_opt\" : feature_selection_opt, \\\n",
    "#                     \"algorithm_opt\" : algorithm_opt}\n",
    "#                 index_counter += 1\n",
    "\n",
    "def construct_config_experiments_scenario_map():\n",
    "    index_counter = 1\n",
    "    for sampling_opt in SAMPLING_OPTIONS:\n",
    "        for algorithm_opt in MODEL_ALGORITHM_OPTIONS:\n",
    "            CONFIG_EXPERIMENTS_SCENARIO_MAP[index_counter] = \\\n",
    "                {\"sampling_opt\" : sampling_opt, \\\n",
    "                \"algorithm_opt\" : algorithm_opt}\n",
    "            index_counter += 1\n",
    "\n",
    "construct_config_experiments_scenario_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMNS FOR TEST EXPERIMENT RESULT\n",
    "COLUMNS_EVALUATION_METRIC_CLASSIFICATION = [\n",
    "    \"rec_score\",\n",
    "    \"prec_score\",\n",
    "    \"F1_score\",\n",
    "    \"acc_score\",\n",
    "    \"auc_score\",\n",
    "    \"mcc_score\",\n",
    "    \"brier_score\",\n",
    "    \"log_loss_score\",\n",
    "    \"balanced_acc_score\"\n",
    "]\n",
    "COLUMNS_EVALUATION_METRIC_REGRESSION = [\n",
    "    \"mean_squared_error_score\",\n",
    "    \"root_mean_squared_error_score\",\n",
    "    \"auc_score\",\n",
    "    \"brier_score\",\n",
    "    \"log_loss_score\",\n",
    "    \"mean_absolute_error_score\",\n",
    "    \"r_squared_score\",\n",
    "    \"mean_absolute_percentage_error_score\"\n",
    "]\n",
    "COLUMNS_SCENARIO_NAME = [\n",
    "    \"sampling_opt\",\n",
    "    # \"feature_selection_opt\",\n",
    "    \"algorithm_opt\"\n",
    "]\n",
    "if (USE_EVALUATION_METRIC_CLASSIFICATION):\n",
    "    COLUMNS_EXPERIMENT_RESULT = [\"case_number\"] + COLUMNS_SCENARIO_NAME + COLUMNS_EVALUATION_METRIC_CLASSIFICATION\n",
    "else:\n",
    "    COLUMNS_EXPERIMENT_RESULT = [\"case_number\"] + COLUMNS_SCENARIO_NAME + COLUMNS_EVALUATION_METRIC_REGRESSION\n",
    "\n",
    "COLUMNS_EXPERIMENT_RESULT_PLAYER_SKILL_PROBS = [\n",
    "    \"feature_selection_opt\",\n",
    "    \"mean_squared_error_score\",\n",
    "    \"root_mean_squared_error_score\",\n",
    "    \"mean_absolute_error_score\",\n",
    "    \"r_squared_score\",\n",
    "    \"mean_absolute_percentage_error_score\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wyscout = PublicWyscoutLoader(root=\"data/wyscout\")\n",
    "api_opta = OptaLoader(root=\"data/opta\")\n",
    "api_statsbomb = StatsBombLoader(root=\"data/statsbomb\", getter=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_id : 0   action_name : pass\n",
      "action_id : 1   action_name : cross\n",
      "action_id : 2   action_name : throw_in\n",
      "action_id : 3   action_name : freekick_crossed\n",
      "action_id : 4   action_name : freekick_short\n",
      "action_id : 5   action_name : corner_crossed\n",
      "action_id : 6   action_name : corner_short\n",
      "action_id : 7   action_name : take_on\n",
      "action_id : 8   action_name : foul\n",
      "action_id : 9   action_name : tackle\n",
      "action_id : 10   action_name : interception\n",
      "action_id : 11   action_name : shot\n",
      "action_id : 12   action_name : shot_penalty\n",
      "action_id : 13   action_name : shot_freekick\n",
      "action_id : 14   action_name : keeper_save\n",
      "action_id : 15   action_name : keeper_claim\n",
      "action_id : 16   action_name : keeper_punch\n",
      "action_id : 17   action_name : keeper_pick_up\n",
      "action_id : 18   action_name : clearance\n",
      "action_id : 19   action_name : bad_touch\n",
      "action_id : 20   action_name : non_action\n",
      "action_id : 21   action_name : dribble\n",
      "action_id : 22   action_name : goalkick\n"
     ]
    }
   ],
   "source": [
    "for idx, action_name in enumerate(actiontypes):\n",
    "    print(f'action_id : {idx}   action_name : {action_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodypart_id : 0   bodypart_name : foot\n",
      "bodypart_id : 1   bodypart_name : head\n",
      "bodypart_id : 2   bodypart_name : other\n",
      "bodypart_id : 3   bodypart_name : head/other\n",
      "bodypart_id : 4   bodypart_name : foot_left\n",
      "bodypart_id : 5   bodypart_name : foot_right\n"
     ]
    }
   ],
   "source": [
    "for idx, bodypart_name in enumerate(bodyparts):\n",
    "    print(f'bodypart_id : {idx}   bodypart_name : {bodypart_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_events_df_to_spadl(events_df, home_team_id, source):\n",
    "    if (source == \"Statsbomb\"):\n",
    "        spadl_events_df = convert_to_actions_statsbomb(events_df, home_team_id)\n",
    "    else:\n",
    "        spadl_events_df = convert_to_actions_wyscout(events_df, home_team_id)\n",
    "    spadl_events_df['time_seconds'] = spadl_events_df['time_seconds'].astype('float64')\n",
    "    spadl_events_df['timestamp'] = pd.to_datetime(spadl_events_df['time_seconds'], unit='s')\n",
    "    return spadl_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO ADD ADDITIONAL INFO IN RAW SPADL DATAFRAME\n",
    "STANDARD_LENGTH_COURT = 105\n",
    "STANDARD_WIDTH_COURT = 68\n",
    "STANDARD_GOALLINE_WIDTH = 7.32\n",
    "STANDARD_LENGTH_COURT_STATSBOMB = 120\n",
    "STANDARD_WIDTH_COURT_STATSBOMB = 80\n",
    "\n",
    "# Helper Functions\n",
    "def filter_out_is_home_team_apply_df(row, home_team_id):\n",
    "    return 1 if row['team_id'] == home_team_id else 0\n",
    "\n",
    "def filter_out_take_on_or_dribble_apply_df(row, take_on_action_id):\n",
    "    return 1 if row['action_id'] == take_on_action_id else 0\n",
    "\n",
    "# Helper functions specific to statsbomb opponent data\n",
    "def calculate_distance_between_two_coordinates(x1, y1, x2, y2):\n",
    "    return math.sqrt(abs(x2-x1) ** 2 + abs(y2-y1) ** 2)\n",
    "\n",
    "def filter_out_non_opponent_coordinate_freeze_frame(freeze_frame_360_list):\n",
    "    if (freeze_frame_360_list == None or not isinstance(freeze_frame_360_list, list)):\n",
    "        return []\n",
    "    return [x for x in freeze_frame_360_list if x['teammate'] == False and x['actor'] == False]\n",
    "\n",
    "def convert_statsbomb_coordinate_to_spadl_coordinate(coordinate_x, coordinate_y):\n",
    "    converted_coordinate_x = (STANDARD_LENGTH_COURT / STANDARD_LENGTH_COURT_STATSBOMB) * coordinate_x\n",
    "    converted_coordinate_y = (STANDARD_WIDTH_COURT / STANDARD_WIDTH_COURT_STATSBOMB) * coordinate_y\n",
    "    return (converted_coordinate_x, converted_coordinate_y)\n",
    "\n",
    "def calculate_distance_opponent_apply_df(row):\n",
    "    freeze_frame_360_opponents = filter_out_non_opponent_coordinate_freeze_frame(row['freeze_frame_360'])\n",
    "    list_distance_opponent = []\n",
    "    for object_loc in freeze_frame_360_opponents:\n",
    "        opponent_x, opponent_y = convert_statsbomb_coordinate_to_spadl_coordinate(object_loc['location'][0], object_loc['location'][1])\n",
    "        distance_opponent = calculate_distance_between_two_coordinates(row['start_x'], row['start_y'], opponent_x, opponent_y)\n",
    "        list_distance_opponent.append(distance_opponent)\n",
    "    return min(list_distance_opponent) if len(list_distance_opponent) > 0 else 0\n",
    "\n",
    "def calculate_num_opponent_closer_goal(start_x, start_y, freeze_frame_360, is_home_team):\n",
    "    freeze_frame_360_opponents = filter_out_non_opponent_coordinate_freeze_frame(freeze_frame_360)\n",
    "    if (is_home_team):\n",
    "        coordinate_x_goal = STANDARD_LENGTH_COURT\n",
    "    else:\n",
    "        coordinate_x_goal = 0\n",
    "    coordinate_y_goal = STANDARD_WIDTH_COURT / 2\n",
    "\n",
    "    num_opponent_closer_to_goal = 0\n",
    "    for object_loc in freeze_frame_360_opponents:\n",
    "        opponent_x, opponent_y = convert_statsbomb_coordinate_to_spadl_coordinate(object_loc['location'][0], object_loc['location'][1])\n",
    "        distance_passer_to_goal = calculate_distance_between_two_coordinates(start_x, start_y, coordinate_x_goal, coordinate_y_goal)\n",
    "        distance_opponent_to_goal = calculate_distance_between_two_coordinates(opponent_x, opponent_y, coordinate_x_goal, coordinate_y_goal)\n",
    "        if (distance_opponent_to_goal < distance_passer_to_goal):\n",
    "            num_opponent_closer_to_goal += 1\n",
    "    return num_opponent_closer_to_goal\n",
    "\n",
    "def calculate_num_opponent_closer_goal_apply_df(row, home_team_id):\n",
    "    return calculate_num_opponent_closer_goal(row['start_x'], row['start_y'], row['freeze_frame_360'], (row['team_id'] == home_team_id))\n",
    "\n",
    "def calculate_num_opponent_in_path(start_x, start_y, freeze_frame_360):\n",
    "    path_distance = 10\n",
    "    freeze_frame_360_opponents = filter_out_non_opponent_coordinate_freeze_frame(freeze_frame_360)\n",
    "    num_opponent_in_path = 0\n",
    "    for object_loc in freeze_frame_360_opponents:\n",
    "        opponent_x, opponent_y = convert_statsbomb_coordinate_to_spadl_coordinate(object_loc['location'][0], object_loc['location'][1])\n",
    "        distance_with_opponent = calculate_distance_between_two_coordinates(start_x, start_y, opponent_x, opponent_y)\n",
    "        if (distance_with_opponent <= path_distance):\n",
    "            num_opponent_in_path += 1\n",
    "    return num_opponent_in_path\n",
    "\n",
    "def calculate_num_opponent_in_path_apply_df(row):\n",
    "    return calculate_num_opponent_in_path(row['start_x'], row['start_y'], row['freeze_frame_360'])\n",
    "\n",
    "def calculate_distance_dribble(coordinate_x, coordinate_y, end_x, end_y):\n",
    "    distance_passing = math.sqrt((abs(end_x - coordinate_x)) ** 2 + (abs(end_y - coordinate_y)) ** 2)\n",
    "    return distance_passing\n",
    "\n",
    "def calculate_distance_dribble_apply_df(row):\n",
    "    return calculate_distance_dribble(row['start_x'], row['start_y'], row['end_x'], row['end_y'])\n",
    "\n",
    "# Add is_home_team column (boolean 0/1)\n",
    "def add_is_home_team_column_to_spadl_df(spadl_df, home_team_id):\n",
    "    spadl_df['is_home_team'] = spadl_df.apply(lambda x : filter_out_is_home_team_apply_df(x, home_team_id), axis=1)\n",
    "    return spadl_df\n",
    "\n",
    "# Add is_take_on column (boolean 0/1)\n",
    "def add_is_take_on_column_to_spadl_df(spadl_df, take_on_action_id):\n",
    "    spadl_df['is_take_on'] = spadl_df.apply(lambda x : filter_out_take_on_or_dribble_apply_df(x, take_on_action_id), axis=1)\n",
    "    return spadl_df\n",
    "\n",
    "# Feature 1 : distance dribble\n",
    "def add_distance_dribble_to_spadl_df(spadl_df):\n",
    "    spadl_df['distance_dribble'] = spadl_df.apply(calculate_distance_dribble_apply_df, axis=1)\n",
    "    return spadl_df\n",
    "\n",
    "# Opponent Feature 1 : distance opponent\n",
    "def add_distance_opponent_column_to_spadl_df(spadl_df):\n",
    "    spadl_df['distance_opponent'] = spadl_df.apply(calculate_distance_opponent_apply_df, axis=1)\n",
    "    return spadl_df\n",
    "\n",
    "# Opponent Feature 2 : opponents closer to goal\n",
    "def add_num_opponent_closer_goal_column_to_spadl_df(spadl_df, home_team_id):\n",
    "    spadl_df['num_opponent_closer_goal'] = spadl_df.apply(lambda x : calculate_num_opponent_closer_goal_apply_df(x, home_team_id), axis=1)\n",
    "    return spadl_df\n",
    "\n",
    "# Opponent Feature 3 : opponents in path\n",
    "def add_num_opponent_in_path_column_to_spadl_df(spadl_df):\n",
    "    spadl_df['num_opponent_in_path'] = spadl_df.apply(calculate_num_opponent_in_path_apply_df, axis=1)\n",
    "    return spadl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all dataset action specific type, export them to csv files\n",
    "# Take_on (action_id = 7), Dribble (action_id = 21)\n",
    "DRIBBLE_ACTION_ID = [7, 21] \n",
    "TAKE_ON_ACTION_ID = 7\n",
    "\n",
    "def collect_raw_dribble_spadl_df(source=\"Wyscout\", period=1):\n",
    "    if source == \"Statsbomb\":\n",
    "        api = api_statsbomb\n",
    "    else:\n",
    "        api = api_wyscout\n",
    "    list_competitions_ids = []\n",
    "    list_game_ids = []\n",
    "\n",
    "    competitions_df = api.competitions()\n",
    "    for _, row in competitions_df.iterrows():\n",
    "        if source == \"Statsbomb\":\n",
    "            if row['competition_gender'] == 'male':\n",
    "                list_competitions_ids.append((row['competition_id'], row['season_id']))\n",
    "        else:\n",
    "            list_competitions_ids.append((row['competition_id'], row['season_id']))\n",
    "        \n",
    "    for competition_id, season_id in list_competitions_ids:\n",
    "        games_df = api.games(competition_id, season_id)\n",
    "        for _, row in games_df.iterrows():\n",
    "            list_game_ids.append((row['game_id'], row['home_team_id'], row['away_team_id']))\n",
    "            \n",
    "    for game_id, home_team_id, away_team_id in list_game_ids:\n",
    "        try:\n",
    "            if (source == \"Statsbomb\"):\n",
    "                this_game_events_df = api.events(game_id, load_360=True)\n",
    "            else:\n",
    "                this_game_events_df = api.events(game_id)\n",
    "            this_game_events_spadl_df = convert_events_df_to_spadl(this_game_events_df, home_team_id, source)\n",
    "\n",
    "            # Add column 360 data into events spadl data (Statsbomb)\n",
    "            if (source == \"Statsbomb\"):\n",
    "                this_game_events_spadl_df = pd.merge(this_game_events_spadl_df, this_game_events_df[[\"event_id\", \"visible_area_360\", \"freeze_frame_360\"]], how=\"inner\", left_on=\"original_event_id\", right_on=\"event_id\")\n",
    "                this_game_events_spadl_df.dropna(subset=[\"freeze_frame_360\"])\n",
    "            \n",
    "            # Filter action id with type dribble only, pick only data from first period\n",
    "            this_game_events_spadl_df = this_game_events_spadl_df[this_game_events_spadl_df['type_id'].isin(DRIBBLE_ACTION_ID)]\n",
    "            # if (period != None):\n",
    "            #     this_game_events_spadl_df = this_game_events_spadl_df[this_game_events_spadl_df['period_id'] == period]\n",
    "            # else:\n",
    "            #     this_game_events_spadl_df = this_game_events_spadl_df[this_game_events_spadl_df['period_id'] == 1]\n",
    "            \n",
    "            # Add additional computed column to support xDribble model\n",
    "            this_game_events_spadl_df = add_is_home_team_column_to_spadl_df(this_game_events_spadl_df, home_team_id)\n",
    "            this_game_events_spadl_df = add_is_take_on_column_to_spadl_df(this_game_events_spadl_df, TAKE_ON_ACTION_ID)\n",
    "            this_game_events_spadl_df = add_distance_dribble_to_spadl_df(this_game_events_spadl_df)\n",
    "            if (source == \"Statsbomb\"):\n",
    "                this_game_events_spadl_df = add_distance_opponent_column_to_spadl_df(this_game_events_spadl_df)\n",
    "                this_game_events_spadl_df = add_num_opponent_closer_goal_column_to_spadl_df(this_game_events_spadl_df, home_team_id)\n",
    "                this_game_events_spadl_df = add_num_opponent_in_path_column_to_spadl_df(this_game_events_spadl_df)\n",
    "                \n",
    "            # Export to external csv iteratively\n",
    "            this_game_events_spadl_df.to_csv(f'data/training_data_xdribble/{game_id}_{home_team_id}_{away_team_id}_xdribble_data.csv')\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f'File 360 data not found {game_id}-{home_team_id}-{away_team_id}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS TO CREATE ALL DATASET PLAYERS\n",
    "def collect_raw_all_players_df(source=\"Wyscout\"):\n",
    "    if (source == \"Statsbomb\"):\n",
    "        api = api_statsbomb\n",
    "    else:\n",
    "        api = api_wyscout\n",
    "    list_competitions_ids = []\n",
    "    list_game_ids = []\n",
    "\n",
    "    competitions_df = api.competitions()\n",
    "    for _, row in competitions_df.iterrows():\n",
    "        if (source == \"Statsbomb\"):\n",
    "            if (row['competition_gender'] == 'male'):\n",
    "                list_competitions_ids.append((row['competition_id'], row['season_id']))\n",
    "        else:\n",
    "            list_competitions_ids.append((row['competition_id'], row['season_id']))\n",
    "        \n",
    "    for competition_id, season_id in list_competitions_ids:\n",
    "        games_df = api.games(competition_id, season_id)\n",
    "        for _, row in games_df.iterrows():\n",
    "            list_game_ids.append((row['game_id'], row['home_team_id'], row['away_team_id']))\n",
    "\n",
    "    for game_id, home_team_id, away_team_id in list_game_ids:\n",
    "        players_df = api.players(game_id)\n",
    "        if (source == \"Statsbomb\"):\n",
    "            players_df.to_csv(f'data/training_data_players_statsbomb/{game_id}_{home_team_id}_{away_team_id}_players_data.csv')\n",
    "        else:\n",
    "            players_df.to_csv(f'data/training_data_players_wyscout/{game_id}_{home_team_id}_{away_team_id}_players_data.csv')\n",
    "\n",
    "def load_and_concat_players_df_from_csv(path_to_raw_players_df):\n",
    "    list_raw_players_df = []\n",
    "    for filename in os.listdir(path_to_raw_players_df):\n",
    "        f = os.path.join(path_to_raw_players_df, filename)\n",
    "        if os.path.isfile(f):\n",
    "            players_df = pd.read_csv(f)\n",
    "            list_raw_players_df.append(players_df)\n",
    "    merged_players_df = pd.concat(list_raw_players_df)\n",
    "    merged_players_df = merged_players_df.drop_duplicates(subset='player_id').reset_index()\n",
    "    return merged_players_df\n",
    "\n",
    "def load_csv_players_data_sofifa(path_to_sofifa_file):\n",
    "    return pd.read_csv(path_to_sofifa_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment it if players dataset already loaded\n",
    "# collect_raw_all_players_df(source=\"Statsbomb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge wyscout player datasets with sofifa datasets by matching string name\n",
    "def create_maps_for_name_matching_scores(list_unique_names_df_1, list_unique_names_df_2):\n",
    "    maps_name_matching_score = {}\n",
    "    for name_1 in list_unique_names_df_1:\n",
    "        for name_2 in list_unique_names_df_2:\n",
    "            maps_name_matching_score[(name_1, name_2)] = fuzz.ratio(name_1, name_2)\n",
    "    return maps_name_matching_score\n",
    "\n",
    "def filter_out_maps_for_name_matching_scores(maps_name_matching, threshold):\n",
    "    filtered_maps_name_matching = {}\n",
    "    for name_1, name_2 in maps_name_matching:\n",
    "        if (maps_name_matching[(name_1, name_2)] >= threshold):\n",
    "            filtered_maps_name_matching[(name_1, name_2)] = maps_name_matching[(name_1, name_2)]\n",
    "    return filtered_maps_name_matching\n",
    "\n",
    "def merge_big_dataframe_wyscout_with_sofifa(big_dataframe_players, sofifa_players_dataset, maps_name_matching_score):\n",
    "    # Preprocess both dataframes and add prefix 1- and 2- to all column names to avoid duplicate column names\n",
    "    big_dataframe_players.dropna(subset=['player_name'], inplace=True)\n",
    "    big_dataframe_players.rename(columns=lambda x: '1-'+x, inplace=True)\n",
    "    sofifa_players_dataset.dropna(subset=['full_name'], inplace=True)\n",
    "    sofifa_players_dataset.rename(columns=lambda x: '2-'+x, inplace=True)\n",
    "    # Merge into new empty dataframe one by one by iterating maps name matching score\n",
    "    big_dataframe_players_with_sofifa = pd.DataFrame(columns=list(big_dataframe_players.columns)+list(sofifa_players_dataset.columns), index=[0])\n",
    "    big_dataframe_players_with_sofifa.reset_index(inplace=True)\n",
    "    for name_1, name_2 in maps_name_matching_score:\n",
    "        row_from_big_dataframe_players = big_dataframe_players[big_dataframe_players['1-player_name'] == name_1].iloc[0]\n",
    "        row_from_sofifa_players_dataset = sofifa_players_dataset[sofifa_players_dataset['2-full_name'] == name_2].iloc[0]\n",
    "        new_row = pd.concat([row_from_big_dataframe_players, row_from_sofifa_players_dataset], axis=0, ignore_index=False)\n",
    "        new_row = pd.DataFrame([new_row]).reset_index()\n",
    "        big_dataframe_players_with_sofifa = pd.concat([big_dataframe_players_with_sofifa, new_row])\n",
    "    # Remove prefix 1- and 2- from final big datasets\n",
    "    big_dataframe_players_with_sofifa.rename(columns=lambda x: x[2:], inplace=True)\n",
    "    return big_dataframe_players_with_sofifa\n",
    "\n",
    "SOURCE_DATA = \"Statsbomb\"\n",
    "if (SOURCE_DATA == \"Statsbomb\"):\n",
    "    DIRECTORY_PLAYERS_CSV_DATAS = \"data/training_data_players_statsbomb\"\n",
    "else:\n",
    "    DIRECTORY_PLAYERS_CSV_DATAS = \"data/training_data_players_wyscout\"\n",
    "DIRECTORY_SOFIFA_CSV_DATAS = \"data/players_skill_dataset/sofifa_dataset_cleaned.csv\"\n",
    "DIRECTORY_WYSCOUT_CSV_DATAS = \"data/players_skill_dataset/wyscout_dataset_cleaned.csv\"\n",
    "DIRECTORY_STATSBOMB_CSV_DATAS = \"data/players_skill_dataset/statsbomb_dataset_cleaned.csv\"\n",
    "DIRECTORY_FINAL_PLAYERS_CSV_DATAS = \"data/players_skill_dataset/final_players_skill_dataset.csv\"\n",
    "\n",
    "# COMMENT BELOW SNIPPET CODES IF FINAL PLAYER DATASETS WITH SKILL ALREADY GENERATED !!\n",
    "# big_dataframe_players = load_and_concat_players_df_from_csv(DIRECTORY_PLAYERS_CSV_DATAS)\n",
    "# if (SOURCE_DATA == \"Statsbomb\"):\n",
    "#     big_dataframe_players.to_csv(DIRECTORY_STATSBOMB_CSV_DATAS)\n",
    "# else:\n",
    "#     big_dataframe_players.to_csv(DIRECTORY_WYSCOUT_CSV_DATAS)\n",
    "# sofifa_players_dataset = load_csv_players_data_sofifa(DIRECTORY_SOFIFA_CSV_DATAS)\n",
    "\n",
    "# maps_name_matching_score = create_maps_for_name_matching_scores(big_dataframe_players['player_name'].unique(), sofifa_players_dataset['full_name'].unique())\n",
    "# maps_name_matching_score = filter_out_maps_for_name_matching_scores(maps_name_matching_score, threshold=80)\n",
    "\n",
    "# big_dataframe_players_with_sofifa = merge_big_dataframe_wyscout_with_sofifa(big_dataframe_players, sofifa_players_dataset, maps_name_matching_score)\n",
    "# big_dataframe_players_with_sofifa.reset_index(inplace=True)\n",
    "# big_dataframe_players_with_sofifa = big_dataframe_players_with_sofifa.drop_duplicates(subset='player_id')\n",
    "# big_dataframe_players_with_sofifa.to_csv(DIRECTORY_FINAL_PLAYERS_CSV_DATAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DRIVER (comment it if csv files already loaded)\n",
    "# collect_raw_dribble_spadl_df(source=\"Statsbomb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv datas already retrieved then concat them into one big dataframe\n",
    "DIRECTORY_XDRIBBLE_CSV_DATAS = \"data/training_data_xdribble\"\n",
    "\n",
    "def load_and_concat_xdribble_df_from_csv():\n",
    "    list_pass_event_df = []\n",
    "    for filename in os.listdir(DIRECTORY_XDRIBBLE_CSV_DATAS):\n",
    "        f = os.path.join(DIRECTORY_XDRIBBLE_CSV_DATAS, filename)\n",
    "        if os.path.isfile(f):\n",
    "            pass_event_df = pd.read_csv(f)\n",
    "            list_pass_event_df.append(pass_event_df)\n",
    "    return pd.concat(list_pass_event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>game_id_x</th>\n",
       "      <th>original_event_id</th>\n",
       "      <th>period_id</th>\n",
       "      <th>time_seconds</th>\n",
       "      <th>team_id_x</th>\n",
       "      <th>player_id</th>\n",
       "      <th>start_x</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_x</th>\n",
       "      <th>...</th>\n",
       "      <th>LWB</th>\n",
       "      <th>LDM</th>\n",
       "      <th>CDM</th>\n",
       "      <th>RDM</th>\n",
       "      <th>RWB</th>\n",
       "      <th>LB</th>\n",
       "      <th>LCB</th>\n",
       "      <th>CB</th>\n",
       "      <th>RCB</th>\n",
       "      <th>RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3788741</td>\n",
       "      <td>bea4235d-7e40-461c-bb82-6d473f5bb324</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>909</td>\n",
       "      <td>8963.0</td>\n",
       "      <td>27.794118</td>\n",
       "      <td>44.070886</td>\n",
       "      <td>31.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>65+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>65+2</td>\n",
       "      <td>68+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>68+2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>3788741</td>\n",
       "      <td>d4a29d8a-01f6-4ddb-87e1-05d429d81662</td>\n",
       "      <td>1</td>\n",
       "      <td>203.0</td>\n",
       "      <td>909</td>\n",
       "      <td>8963.0</td>\n",
       "      <td>23.558824</td>\n",
       "      <td>47.944304</td>\n",
       "      <td>24.264706</td>\n",
       "      <td>...</td>\n",
       "      <td>65+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>65+2</td>\n",
       "      <td>68+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>68+2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288</td>\n",
       "      <td>3788741</td>\n",
       "      <td>ce7d0f67-d9af-495f-83fe-d498f0ce0aed</td>\n",
       "      <td>1</td>\n",
       "      <td>657.0</td>\n",
       "      <td>909</td>\n",
       "      <td>8963.0</td>\n",
       "      <td>14.735294</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>13.235294</td>\n",
       "      <td>...</td>\n",
       "      <td>65+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>65+2</td>\n",
       "      <td>68+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>68+2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>328</td>\n",
       "      <td>3788741</td>\n",
       "      <td>e43c65e9-33f1-4cda-93fe-9a918eb480ca</td>\n",
       "      <td>1</td>\n",
       "      <td>761.0</td>\n",
       "      <td>909</td>\n",
       "      <td>8963.0</td>\n",
       "      <td>43.323529</td>\n",
       "      <td>60.167089</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>65+2</td>\n",
       "      <td>68+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>68+2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332</td>\n",
       "      <td>3788741</td>\n",
       "      <td>c3bcba92-1a6f-492e-aced-8a64df6a52ea</td>\n",
       "      <td>1</td>\n",
       "      <td>767.0</td>\n",
       "      <td>909</td>\n",
       "      <td>8963.0</td>\n",
       "      <td>44.117647</td>\n",
       "      <td>62.491139</td>\n",
       "      <td>42.176471</td>\n",
       "      <td>...</td>\n",
       "      <td>65+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>67+2</td>\n",
       "      <td>65+2</td>\n",
       "      <td>68+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>73+2</td>\n",
       "      <td>68+2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x  game_id_x                     original_event_id  period_id  \\\n",
       "0             1    3788741  bea4235d-7e40-461c-bb82-6d473f5bb324          1   \n",
       "1            71    3788741  d4a29d8a-01f6-4ddb-87e1-05d429d81662          1   \n",
       "2           288    3788741  ce7d0f67-d9af-495f-83fe-d498f0ce0aed          1   \n",
       "3           328    3788741  e43c65e9-33f1-4cda-93fe-9a918eb480ca          1   \n",
       "4           332    3788741  c3bcba92-1a6f-492e-aced-8a64df6a52ea          1   \n",
       "\n",
       "   time_seconds  team_id_x  player_id    start_x    start_y      end_x  ...  \\\n",
       "0           2.0        909     8963.0  27.794118  44.070886  31.058824  ...   \n",
       "1         203.0        909     8963.0  23.558824  47.944304  24.264706  ...   \n",
       "2         657.0        909     8963.0  14.735294  40.800000  13.235294  ...   \n",
       "3         761.0        909     8963.0  43.323529  60.167089  48.000000  ...   \n",
       "4         767.0        909     8963.0  44.117647  62.491139  42.176471  ...   \n",
       "\n",
       "    LWB   LDM   CDM   RDM   RWB    LB   LCB    CB   RCB    RB  \n",
       "0  65+2  67+2  67+2  67+2  65+2  68+2  73+2  73+2  73+2  68+2  \n",
       "1  65+2  67+2  67+2  67+2  65+2  68+2  73+2  73+2  73+2  68+2  \n",
       "2  65+2  67+2  67+2  67+2  65+2  68+2  73+2  73+2  73+2  68+2  \n",
       "3  65+2  67+2  67+2  67+2  65+2  68+2  73+2  73+2  73+2  68+2  \n",
       "4  65+2  67+2  67+2  67+2  65+2  68+2  73+2  73+2  73+2  68+2  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JOIN ALREADY CONSTRUCTED PLAYER SKILLS DATASET WITH ORIGIN EVENT DATASET WYSCOUT\n",
    "player_skills_dataset = pd.read_csv(DIRECTORY_FINAL_PLAYERS_CSV_DATAS)\n",
    "big_dataframe_xdribble_model = load_and_concat_xdribble_df_from_csv()\n",
    "big_dataframe_xdribble_model = big_dataframe_xdribble_model.merge(player_skills_dataset, how='inner',on='player_id')\n",
    "big_dataframe_xdribble_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>level_0</th>\n",
       "      <th>dex</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>game_id</th>\n",
       "      <th>team_id</th>\n",
       "      <th>player_id</th>\n",
       "      <th>...</th>\n",
       "      <th>LB</th>\n",
       "      <th>LCB</th>\n",
       "      <th>CB</th>\n",
       "      <th>RCB</th>\n",
       "      <th>RB</th>\n",
       "      <th>statistic_total_success_action</th>\n",
       "      <th>statistic_total_action</th>\n",
       "      <th>statistic_success_action_probs</th>\n",
       "      <th>statistic_success_action_among_players</th>\n",
       "      <th>statistic_success_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15946.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>3501.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66+3</td>\n",
       "      <td>58+3</td>\n",
       "      <td>58+3</td>\n",
       "      <td>58+3</td>\n",
       "      <td>66+3</td>\n",
       "      <td>4317</td>\n",
       "      <td>4426</td>\n",
       "      <td>0.975373</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.002367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15946.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>5203.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78+3</td>\n",
       "      <td>82+3</td>\n",
       "      <td>82+3</td>\n",
       "      <td>82+3</td>\n",
       "      <td>78+3</td>\n",
       "      <td>23802</td>\n",
       "      <td>23874</td>\n",
       "      <td>0.996984</td>\n",
       "      <td>0.013382</td>\n",
       "      <td>0.013341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15946.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>5211.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84+3</td>\n",
       "      <td>78+3</td>\n",
       "      <td>78+3</td>\n",
       "      <td>78+3</td>\n",
       "      <td>84+3</td>\n",
       "      <td>15883</td>\n",
       "      <td>16010</td>\n",
       "      <td>0.992067</td>\n",
       "      <td>0.008930</td>\n",
       "      <td>0.008859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15946.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>5213.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78+3</td>\n",
       "      <td>84+3</td>\n",
       "      <td>84+3</td>\n",
       "      <td>84+3</td>\n",
       "      <td>78+3</td>\n",
       "      <td>18196</td>\n",
       "      <td>18222</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>0.010230</td>\n",
       "      <td>0.010215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15946.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>5246.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66+5</td>\n",
       "      <td>63+5</td>\n",
       "      <td>63+5</td>\n",
       "      <td>63+5</td>\n",
       "      <td>66+5</td>\n",
       "      <td>5791</td>\n",
       "      <td>6134</td>\n",
       "      <td>0.944082</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.003074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  level_0  dex  index  Unnamed: 0  \\\n",
       "0             0             1             1        0    0    0.0         0.0   \n",
       "1             1             2             2        0    0    1.0         1.0   \n",
       "2             2             3             3        0    0    2.0         2.0   \n",
       "3             3             4             4        0    0    3.0         3.0   \n",
       "4             4             5             5        0    0    4.0         4.0   \n",
       "\n",
       "   game_id  team_id  player_id  ...    LB   LCB    CB   RCB    RB  \\\n",
       "0  15946.0    217.0     3501.0  ...  66+3  58+3  58+3  58+3  66+3   \n",
       "1  15946.0    217.0     5203.0  ...  78+3  82+3  82+3  82+3  78+3   \n",
       "2  15946.0    217.0     5211.0  ...  84+3  78+3  78+3  78+3  84+3   \n",
       "3  15946.0    217.0     5213.0  ...  78+3  84+3  84+3  84+3  78+3   \n",
       "4  15946.0    217.0     5246.0  ...  66+5  63+5  63+5  63+5  66+5   \n",
       "\n",
       "  statistic_total_success_action  statistic_total_action  \\\n",
       "0                           4317                    4426   \n",
       "1                          23802                   23874   \n",
       "2                          15883                   16010   \n",
       "3                          18196                   18222   \n",
       "4                           5791                    6134   \n",
       "\n",
       "   statistic_success_action_probs statistic_success_action_among_players  \\\n",
       "0                        0.975373                               0.002427   \n",
       "1                        0.996984                               0.013382   \n",
       "2                        0.992067                               0.008930   \n",
       "3                        0.998573                               0.010230   \n",
       "4                        0.944082                               0.003256   \n",
       "\n",
       "  statistic_success_total  \n",
       "0                0.002367  \n",
       "1                0.013341  \n",
       "2                0.008859  \n",
       "3                0.010215  \n",
       "4                0.003074  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD PROBABILITY SKILL DATASETS FOR DRIBBLING EVENT\n",
    "DIRECTORY_PLAYER_SKILLS_PROBABILITIES_DATAS = \"data/model_xdribble/xdribble_player_skill_probs_dataset.csv\"\n",
    "\n",
    "xdribble_player_skill_probs_df = pd.read_csv(DIRECTORY_PLAYER_SKILLS_PROBABILITIES_DATAS)\n",
    "# Filtering outliers data based on DBSCAN Plot Image (see generator code)\n",
    "xdribble_player_skill_probs_df = xdribble_player_skill_probs_df[(xdribble_player_skill_probs_df[\"statistic_success_action_probs\"] >= 0.76) & (xdribble_player_skill_probs_df[\"statistic_success_action_among_players\"] <= 0.015)]\n",
    "xdribble_player_skill_probs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_x</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_x</th>\n",
       "      <th>end_y</th>\n",
       "      <th>result_id</th>\n",
       "      <th>distance_dribble</th>\n",
       "      <th>distance_opponent</th>\n",
       "      <th>num_opponent_closer_goal</th>\n",
       "      <th>num_opponent_in_path</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>long_shots</th>\n",
       "      <th>aggression</th>\n",
       "      <th>interceptions</th>\n",
       "      <th>positioning</th>\n",
       "      <th>vision</th>\n",
       "      <th>penalties</th>\n",
       "      <th>composure</th>\n",
       "      <th>marking</th>\n",
       "      <th>standing_tackle</th>\n",
       "      <th>sliding_tackle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.794118</td>\n",
       "      <td>44.070886</td>\n",
       "      <td>31.058824</td>\n",
       "      <td>42.693671</td>\n",
       "      <td>1</td>\n",
       "      <td>3.543307</td>\n",
       "      <td>17.081977</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.558824</td>\n",
       "      <td>47.944304</td>\n",
       "      <td>24.264706</td>\n",
       "      <td>49.063291</td>\n",
       "      <td>1</td>\n",
       "      <td>1.323028</td>\n",
       "      <td>14.231871</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.735294</td>\n",
       "      <td>40.800000</td>\n",
       "      <td>13.235294</td>\n",
       "      <td>48.202532</td>\n",
       "      <td>1</td>\n",
       "      <td>7.552978</td>\n",
       "      <td>11.364664</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.323529</td>\n",
       "      <td>60.167089</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>62.663291</td>\n",
       "      <td>1</td>\n",
       "      <td>5.300981</td>\n",
       "      <td>22.928909</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.117647</td>\n",
       "      <td>62.491139</td>\n",
       "      <td>42.176471</td>\n",
       "      <td>62.491139</td>\n",
       "      <td>1</td>\n",
       "      <td>1.941176</td>\n",
       "      <td>29.146285</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start_x    start_y      end_x      end_y  result_id  distance_dribble  \\\n",
       "0  27.794118  44.070886  31.058824  42.693671          1          3.543307   \n",
       "1  23.558824  47.944304  24.264706  49.063291          1          1.323028   \n",
       "2  14.735294  40.800000  13.235294  48.202532          1          7.552978   \n",
       "3  43.323529  60.167089  48.000000  62.663291          1          5.300981   \n",
       "4  44.117647  62.491139  42.176471  62.491139          1          1.941176   \n",
       "\n",
       "   distance_opponent  num_opponent_closer_goal  num_opponent_in_path   age  \\\n",
       "0          17.081977                        10                     0  22.0   \n",
       "1          14.231871                         5                     0  22.0   \n",
       "2          11.364664                         3                     0  22.0   \n",
       "3          22.928909                         7                     0  22.0   \n",
       "4          29.146285                         8                     0  22.0   \n",
       "\n",
       "   ...  long_shots  aggression  interceptions  positioning  vision  penalties  \\\n",
       "0  ...        42.0        81.0           71.0         42.0    49.0       52.0   \n",
       "1  ...        42.0        81.0           71.0         42.0    49.0       52.0   \n",
       "2  ...        42.0        81.0           71.0         42.0    49.0       52.0   \n",
       "3  ...        42.0        81.0           71.0         42.0    49.0       52.0   \n",
       "4  ...        42.0        81.0           71.0         42.0    49.0       52.0   \n",
       "\n",
       "   composure  marking  standing_tackle  sliding_tackle  \n",
       "0       59.0     69.0             82.0            79.0  \n",
       "1       59.0     69.0             82.0            79.0  \n",
       "2       59.0     69.0             82.0            79.0  \n",
       "3       59.0     69.0             82.0            79.0  \n",
       "4       59.0     69.0             82.0            79.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SELECT ONLY FEATURED COLUMN FROM BIG DATASETS\n",
    "features_column_included = [\"start_x\", \"start_y\", \"end_x\", \"end_y\", \"distance_opponent\", \n",
    "                            \"num_opponent_closer_goal\", \"num_opponent_in_path\", \"result_id\", \"distance_dribble\"]\n",
    "player_skills_column_included = [\"acceleration\", \"aggression\", \"agility\", \"balance\", \"ball_control\",\n",
    "                                 \"composure\", \"crossing\", \"curve\", \"dribbling\", \"finishing\",\n",
    "                                 \"freekick_accuracy\", \"heading_accuracy\", \"interceptions\", \"jumping\", \"long_passing\",\n",
    "                                 \"long_shots\", \"marking\", \"penalties\", \"positioning\", \"reactions\",\n",
    "                                 \"shot_power\", \"sliding_tackle\", \"sprint_speed\", \"stamina\", \"short_passing\",\n",
    "                                 \"standing_tackle\", \"strength\", \"vision\", \"volleys\"]\n",
    "player_attribute_column_included = [\"height_cm\", \"weight_kgs\", \"age\"]\n",
    "\n",
    "big_dataframe_xdribble_model = big_dataframe_xdribble_model[[c for c in big_dataframe_xdribble_model.columns if c in (features_column_included + player_skills_column_included + player_attribute_column_included)]]\n",
    "big_dataframe_xdribble_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 1 : Random Oversample Function\n",
    "def training_data_random_oversampled(X_train, Y_train):\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, Y_resampled = ros.fit_resample(X_train, Y_train)\n",
    "    return (X_resampled, Y_resampled)\n",
    "\n",
    "# CASE 2 : Random Undersample Function\n",
    "def training_data_random_undersampled(X_train, Y_train):\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_resampled, Y_resampled = rus.fit_resample(X_train, Y_train)\n",
    "    return (X_resampled, Y_resampled)\n",
    "\n",
    "# CASE 3 : Random SMOTE Oversample Function\n",
    "def training_data_smote_oversampled(X_train, Y_train):\n",
    "    X_resampled, Y_resampled = SMOTE().fit_resample(X_train, Y_train)\n",
    "    return (X_resampled, Y_resampled)\n",
    "\n",
    "# V CASE 1 : Feature Selection - Pearson Coefficient\n",
    "def filter_columns_feature_selection_pearson(X_train, Y_train, columns_considered, threshold):\n",
    "    new_columns_after_selection = []\n",
    "    for _, skill in enumerate(columns_considered):\n",
    "        correlation_value, _ = pearsonr(X_train[skill], Y_train)\n",
    "        if correlation_value >= threshold:\n",
    "            new_columns_after_selection.append(skill)\n",
    "    return new_columns_after_selection\n",
    "\n",
    "def training_data_feature_selection_pearson(X_train, Y_train, columns_considered, threshold):\n",
    "    columns_selected = filter_columns_feature_selection_pearson(X_train, Y_train, columns_considered, threshold)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# V CASE 2 : Feature Selection - Chi Square\n",
    "def filter_columns_feature_selection_chisquare(X_train, Y_train, columns_considered, num_of_features):\n",
    "    chi2_selector = SelectKBest(chi2, k=num_of_features) \n",
    "    df_feature = X_train[columns_considered]\n",
    "    chi2_selector.fit(df_feature, Y_train)\n",
    "    cols = chi2_selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_chisquare(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_chisquare(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# V CASE 3 : Feature Selection - Mutual Information\n",
    "def filter_columns_feature_selection_mutualinf(X_train, Y_train, columns_considered, num_of_features):\n",
    "    mi_selector = SelectKBest(mutual_info_classif, k=num_of_features) \n",
    "    df_feature = X_train[columns_considered]\n",
    "    mi_selector.fit(df_feature, Y_train)\n",
    "    cols = mi_selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_mutualinf(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_mutualinf(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# V CASE 4 : Feature Selection - mRMR Selection\n",
    "def filter_columns_feature_selection_mrmr(X_train, Y_train, columns_considered, num_of_features):\n",
    "    df_feature = X_train[columns_considered]\n",
    "    selected_features = mrmr_classif(X=df_feature, y=Y_train, K=num_of_features)\n",
    "    return selected_features\n",
    "\n",
    "def training_data_feature_selection_mrmr(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_mrmr(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# X CASE 5 : Feature Selection - Sequential Forward Selection (SFS)\n",
    "def filter_columns_feature_selection_sfs(X_train, Y_train, columns_considered, num_of_features):\n",
    "    rf = RandomForestClassifier()\n",
    "    sfs = SequentialFeatureSelector(rf, n_features_to_select=num_of_features, direction='forward')\n",
    "    df_feature = X_train[columns_considered]\n",
    "    sfs.fit(df_feature, Y_train)\n",
    "    cols = sfs.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_sfs(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_sfs(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# X CASE 6 : Feature Selection - Sequential Backward Elimination (SBE)\n",
    "def filter_columns_feature_selection_sbe(X_train, Y_train, columns_considered, num_of_features):\n",
    "    rf = RandomForestClassifier()\n",
    "    sfs = SequentialFeatureSelector(rf, n_features_to_select=num_of_features, direction='backward')\n",
    "    df_feature = X_train[columns_considered]\n",
    "    sfs.fit(df_feature, Y_train)\n",
    "    cols = sfs.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_sbe(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_sbe(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# X CASE 7 : Feature Selection - Recursive Feature Elimination\n",
    "def filter_columns_feature_selection_rfe(X_train, Y_train, columns_considered, num_of_features):\n",
    "    estimator = LinearSVR()\n",
    "    selector = RFECV(estimator, step=1, cv=num_of_features)\n",
    "    df_feature = X_train[columns_considered]\n",
    "    selector.fit(df_feature, Y_train)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_rfe(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_rfe(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# V CASE 8 : Feature Selection - Random Forest Embedded (rfembedded)\n",
    "def filter_columns_feature_selection_rfembedded(X_train, Y_train, columns_considered, num_of_features):\n",
    "    estimator = RandomForestClassifier()\n",
    "    selector = SelectFromModel(estimator=estimator, max_features=num_of_features)\n",
    "    df_feature = X_train[columns_considered]\n",
    "    selector.fit(df_feature, Y_train)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_rfembedded(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_rfembedded(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# V CASE 9 : Feature Selection - LASSO\n",
    "def filter_columns_feature_selection_lasso(X_train, Y_train, columns_considered, num_of_features):\n",
    "    estimator = LogisticRegression(penalty='l2', C=0.5, solver='newton-cholesky')\n",
    "    selector = SelectFromModel(estimator=estimator, max_features=num_of_features)\n",
    "    df_feature = X_train[columns_considered]\n",
    "    selector.fit(df_feature, Y_train)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_lasso(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_lasso(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# CASE 1 : Train with model XGBRegressor\n",
    "def fit_and_train_with_model_xgbregressor(X_train, Y_train):\n",
    "    model = XGBRegressor(objective=\"reg:logistic\")\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# CASE 2 : Train with model RandomForestRegressor\n",
    "def fit_and_train_with_model_rfregressor(X_train, Y_train):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# CASE 3 : Train with model LogisticRegression\n",
    "def fit_and_train_with_model_logregression(X_train, Y_train):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# CASE 4 : Train with model XGBClassifier\n",
    "def fit_and_train_with_model_xgbclassifier(X_train, Y_train):\n",
    "    # model = XGBClassifier(n_estimators=50, max_depth=3, n_jobs=-3, verbosity=1, enable_categorical=True)\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# CASE 5 : Train with model Catboost Classifier \n",
    "def fit_and_train_with_model_catboostclassifier(X_train, Y_train):\n",
    "    # model = CatBoostClassifier(n_estimators=50, max_depth=3, verbose=1)\n",
    "    model = CatBoostClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model\n",
    "\n",
    "# CASE 6 : Train with model RandomForest Classifier\n",
    "def fit_and_train_with_model_rfclassifier(X_train, Y_train):\n",
    "    # model = RandomForestClassifier(n_estimators=50, max_depth=3, n_jobs=-3, verbose=1)\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    79648\n",
      "0     1162\n",
      "Name: result_id, dtype: int64\n",
      "1    79648\n",
      "0     1162\n",
      "Name: result_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# FEATURE PREPROCESSING BIG DATASETS AND CREATE XGBOOST MODEL\n",
    "# 1. Change all numeric columns with MinMaxScaler\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "columns_minmax_scaler = [\"start_x\", \"start_y\", \"end_x\", \"end_y\", \"distance_opponent\", \n",
    "                        \"num_opponent_closer_goal\", \"num_opponent_in_path\", \"distance_dribble\"]\n",
    "# Store Description for all Numeric Columns in External CSV\n",
    "df_description_numeric = big_dataframe_xdribble_model[columns_minmax_scaler].describe()\n",
    "filename = 'xdribble_description_numeric_data.csv'\n",
    "directory_model = \"data/model_xdribble/\"\n",
    "df_description_numeric.to_csv(directory_model + filename)\n",
    "# Preprocess to minmax scaler\n",
    "big_dataframe_xdribble_model[columns_minmax_scaler] = scaler.fit_transform(big_dataframe_xdribble_model[columns_minmax_scaler])\n",
    "\n",
    "# 2. Check if data is unbalanced. If it is unbalanced, then do method to oversize the sample\n",
    "print(big_dataframe_xdribble_model['result_id'].value_counts())\n",
    "\n",
    "# 3. Change result_id label into float64 type\n",
    "# big_dataframe_xdribble_model['result_id'] = big_dataframe_xdribble_model['result_id'].astype('float64')\n",
    "\n",
    "# 4. Remove dataframe instead of having result_id (0,1) --> (fail, success)\n",
    "big_dataframe_xdribble_model = big_dataframe_xdribble_model[big_dataframe_xdribble_model['result_id'].isin([0,1])]\n",
    "print(big_dataframe_xdribble_model['result_id'].value_counts())\n",
    "\n",
    "# 5. Split train data and test data from Big Datasets\n",
    "all_feature_columns = columns_minmax_scaler\n",
    "X_train = big_dataframe_xdribble_model[all_feature_columns]\n",
    "Y_train = big_dataframe_xdribble_model[\"result_id\"]\n",
    "\n",
    "# Empty dataframe for saving test result\n",
    "empty_test_result = pd.DataFrame(columns=COLUMNS_EXPERIMENT_RESULT, index=[0])\n",
    "\n",
    "for case_number in sorted(list(CONFIG_EXPERIMENTS_SCENARIO_MAP.keys())):\n",
    "    sampling_opt = CONFIG_EXPERIMENTS_SCENARIO_MAP[case_number][\"sampling_opt\"]\n",
    "    # feature_selection_opt = CONFIG_EXPERIMENTS_SCENARIO_MAP[case_number][\"feature_selection_opt\"]\n",
    "    algorithm_opt = CONFIG_EXPERIMENTS_SCENARIO_MAP[case_number][\"algorithm_opt\"]\n",
    "\n",
    "    # 6. Do oversampling/undersampling and feature selection at same time\n",
    "    if sampling_opt == \"none\":\n",
    "        X_resampled, Y_resampled = X_train, Y_train\n",
    "    else:\n",
    "        X_resampled, Y_resampled = globals()[\"training_data_\" + sampling_opt](X_train, Y_train)\n",
    "    # if feature_selection_opt == \"pearson\":\n",
    "    #     threshold = 0.5\n",
    "    #     X_feature_sel, Y_feature_sel = globals()[\"training_data_feature_selection_\" + feature_selection_opt](X_resampled, Y_resampled, player_skills_column_included, threshold)\n",
    "    # else:\n",
    "    #     num_of_features = 10\n",
    "    #     X_feature_sel, Y_feature_sel = globals()[\"training_data_feature_selection_\" + feature_selection_opt](X_resampled, Y_resampled, player_skills_column_included, num_of_features)\n",
    "\n",
    "    # 7. Do train_test_split on training data\n",
    "    X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_resampled, Y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 8. Train Model\n",
    "    model = globals()[\"fit_and_train_with_model_\" + algorithm_opt](X_train_split, y_train_split)\n",
    "\n",
    "    # 9. Predict Testing Data\n",
    "    y_predict = model.predict(X_test_split)\n",
    "\n",
    "    # 10. Save test result experiment\n",
    "    if (USE_EVALUATION_METRIC_CLASSIFICATION):\n",
    "        rec_score = recall_score(y_test_split, y_predict)\n",
    "        prec_score = precision_score(y_test_split, y_predict)\n",
    "        F1_score = f1_score(y_test_split, y_predict)\n",
    "        acc_score = accuracy_score(y_test_split, y_predict)\n",
    "        auc_score = roc_auc_score(y_test_split, y_predict)\n",
    "        mcc_score = matthews_corrcoef(y_test_split, y_predict)\n",
    "        brier_score = brier_score_loss(y_test_split, y_predict)\n",
    "        log_loss_score = log_loss(y_test_split, y_predict)\n",
    "        balanced_acc_score = balanced_accuracy_score(y_test_split, y_predict)\n",
    "    else:\n",
    "        mean_squared_error_score = mean_squared_error(y_test_split, y_predict)\n",
    "        root_mean_squared_error_score = mean_squared_error(y_test_split, y_predict, squared=False)\n",
    "        auc_score = roc_auc_score(y_test_split, y_predict)\n",
    "        brier_score = brier_score_loss(y_test_split, y_predict)\n",
    "        log_loss_score = log_loss(y_test_split, y_predict)\n",
    "        mean_absolute_error_score = mean_absolute_error(y_test_split, y_predict)\n",
    "        r_squared_score = r2_score(y_test_split, y_predict)\n",
    "        mean_absolute_percentage_error_score = mean_absolute_percentage_error(y_test_split, y_predict)\n",
    "\n",
    "    maps_new_row = {}\n",
    "    if USE_EVALUATION_METRIC_CLASSIFICATION:\n",
    "        eval_metrics_column = COLUMNS_EVALUATION_METRIC_CLASSIFICATION\n",
    "    else:\n",
    "        eval_metrics_column = COLUMNS_EVALUATION_METRIC_REGRESSION\n",
    "    for column in COLUMNS_EXPERIMENT_RESULT:\n",
    "        if column not in eval_metrics_column:\n",
    "            if column == \"case_number\":\n",
    "                maps_new_row[\"case_number\"] = case_number\n",
    "            elif column in COLUMNS_SCENARIO_NAME:\n",
    "                maps_new_row[column] = globals()[column]\n",
    "        else:\n",
    "            maps_new_row[column] = globals()[column]     \n",
    "    new_row = pd.DataFrame(maps_new_row, index=[0])\n",
    "    empty_test_result = pd.concat([new_row, empty_test_result.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "    # 11. Save model to external file\n",
    "    filename = f'xdribble_model_case_{case_number}.sav'\n",
    "    directory_model = \"data/model_xdribble/\"\n",
    "    pickle.dump(model, open(directory_model + filename, 'wb'))\n",
    "\n",
    "# 12. Save test result experiment to external file\n",
    "filename = 'xdribble_test_model_experiment_result.csv'\n",
    "directory_model = \"data/model_xdribble/\"\n",
    "empty_test_result.to_csv(directory_model + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 1 : Feature Selection for Regression - Mutual Information\n",
    "def filter_columns_feature_selection_reg_mutualinf(X_train, Y_train, columns_considered, num_of_features):\n",
    "    mi_selector = SelectKBest(mutual_info_regression, k=num_of_features) \n",
    "    df_feature = X_train[columns_considered]\n",
    "    mi_selector.fit(df_feature, Y_train)\n",
    "    cols = mi_selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_reg_mutualinf(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_reg_mutualinf(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)\n",
    "\n",
    "# CASE 2 : Feature Selection for Regression - Pearson Coefficient\n",
    "def filter_columns_feature_selection_reg_pearson(X_train, Y_train, columns_considered, num_of_features):\n",
    "    mi_selector = SelectKBest(f_regression, k=num_of_features) \n",
    "    df_feature = X_train[columns_considered]\n",
    "    mi_selector.fit(df_feature, Y_train)\n",
    "    cols = mi_selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_reg_pearson(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_selected = filter_columns_feature_selection_reg_pearson(X_train, Y_train, columns_considered, num_of_features)\n",
    "    columns_omitted = [x for x in columns_considered if x not in columns_selected]\n",
    "    final_columns = [x for x in list(X_train.columns) if x not in columns_omitted]\n",
    "    print(final_columns)\n",
    "    return (X_train[final_columns], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acceleration', 'agility', 'curve', 'dribbling', 'finishing', 'long_shots', 'penalties', 'positioning', 'sprint_speed', 'volleys', 'height_cm', 'weight_kgs', 'age']\n",
      "['ball_control', 'crossing', 'dribbling', 'finishing', 'interceptions', 'marking', 'positioning', 'sliding_tackle', 'standing_tackle', 'volleys', 'height_cm', 'weight_kgs', 'age']\n"
     ]
    }
   ],
   "source": [
    "# CREATE REGRESSION XGBOOST MODEL FOR PLAYER SKILL PROBABILITIES DATASET \n",
    "# 1. Change all numeric columns with MinMaxScaler\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "columns_minmax_scaler = player_skills_column_included + player_attribute_column_included + [\"statistic_success_action_probs\"]\n",
    "# Store Description for all Numeric Columns in External CSV\n",
    "df_description_numeric = xdribble_player_skill_probs_df[columns_minmax_scaler].describe()\n",
    "filename = 'xdribble_skill_probs_description_numeric_data.csv'\n",
    "directory_model = \"data/model_xdribble/\"\n",
    "df_description_numeric.to_csv(directory_model + filename)\n",
    "# Preprocess to minmax scaler\n",
    "xdribble_player_skill_probs_df[columns_minmax_scaler] = scaler.fit_transform(xdribble_player_skill_probs_df[columns_minmax_scaler])\n",
    "\n",
    "# 2. Change type of minmax column as float64\n",
    "xdribble_player_skill_probs_df[columns_minmax_scaler] = xdribble_player_skill_probs_df[columns_minmax_scaler].astype('float64')\n",
    "\n",
    "# 3. Split train data and test data from Big Datasets\n",
    "all_feature_columns = player_skills_column_included + player_attribute_column_included\n",
    "X_train = xdribble_player_skill_probs_df[all_feature_columns]\n",
    "Y_train = xdribble_player_skill_probs_df[\"statistic_success_action_probs\"]\n",
    "\n",
    "# Empty dataframe for saving test result\n",
    "empty_test_result = pd.DataFrame(columns=COLUMNS_EXPERIMENT_RESULT_PLAYER_SKILL_PROBS, index=[0])\n",
    "\n",
    "case_number = 1\n",
    "for feature_selection_opt in FEATURE_SELECTION_OPTIONS_FOR_PLAYER_PROBS:\n",
    "    # 4. Do feature selection on train data\n",
    "    num_of_features = 10\n",
    "    X_feature_sel, Y_feature_sel = globals()[\"training_data_feature_selection_reg_\" + feature_selection_opt](X_train, Y_train, player_skills_column_included, num_of_features)\n",
    "\n",
    "    # 5. Do train_test_split on training data\n",
    "    X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_feature_sel, Y_feature_sel, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 6. Train Model with XGBRegressor\n",
    "    model = fit_and_train_with_model_xgbregressor(X_train_split, y_train_split)\n",
    "\n",
    "    # 7. Predict Testing Data\n",
    "    y_predict = model.predict(X_test_split)\n",
    "\n",
    "    # 8. Save test result experiment\n",
    "    mean_squared_error_score = mean_squared_error(y_test_split, y_predict)\n",
    "    root_mean_squared_error_score = mean_squared_error(y_test_split, y_predict, squared=False)\n",
    "    mean_absolute_error_score = mean_absolute_error(y_test_split, y_predict)\n",
    "    r_squared_score = r2_score(y_test_split, y_predict)\n",
    "    mean_absolute_percentage_error_score = mean_absolute_percentage_error(y_test_split, y_predict)\n",
    "\n",
    "    maps_new_row = {}\n",
    "    eval_metrics_column = COLUMNS_EVALUATION_METRIC_REGRESSION\n",
    "    for column in COLUMNS_EXPERIMENT_RESULT_PLAYER_SKILL_PROBS:\n",
    "        if column not in eval_metrics_column:\n",
    "            if column == \"feature_selection_opt\":\n",
    "                maps_new_row[\"feature_selection_opt\"] = feature_selection_opt\n",
    "        else:\n",
    "            maps_new_row[column] = globals()[column]     \n",
    "    new_row = pd.DataFrame(maps_new_row, index=[0])\n",
    "    empty_test_result = pd.concat([new_row, empty_test_result.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "    # 9. Save model to external file\n",
    "    filename = f'xdribble_player_skill_probs_model_case_{case_number}.sav'\n",
    "    directory_model = \"data/model_xdribble/\"\n",
    "    pickle.dump(model, open(directory_model + filename, 'wb'))\n",
    "\n",
    "    case_number += 1\n",
    "\n",
    "# 10. Save test result experiment to external file\n",
    "filename = 'xdribble_player_skill_probs_model_experiment_result.csv'\n",
    "directory_model = \"data/model_xdribble/\"\n",
    "empty_test_result.to_csv(directory_model + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
