{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "from socceraction.data.wyscout import PublicWyscoutLoader\n",
    "from socceraction.spadl.wyscout import convert_to_actions\n",
    "from socceraction.data.opta import OptaLoader\n",
    "from socceraction.data.statsbomb import StatsBombLoader\n",
    "from socceraction.spadl.config import actiontypes, bodyparts\n",
    "import socceraction.spadl as spadl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from rapidfuzz import fuzz\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_selection import r_regression, SelectKBest, chi2, mutual_info_classif, SequentialFeatureSelector, RFECV, SelectFromModel\n",
    "from scipy.stats import pearsonr, chisquare\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import Lasso, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wyscout = PublicWyscoutLoader(root=\"data/wyscout\")\n",
    "api_opta = OptaLoader(root=\"data/opta\")\n",
    "# api_statsbomb = StatsBombLoader(root=\"data/statsbomb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_id : 0   action_name : pass\n",
      "action_id : 1   action_name : cross\n",
      "action_id : 2   action_name : throw_in\n",
      "action_id : 3   action_name : freekick_crossed\n",
      "action_id : 4   action_name : freekick_short\n",
      "action_id : 5   action_name : corner_crossed\n",
      "action_id : 6   action_name : corner_short\n",
      "action_id : 7   action_name : take_on\n",
      "action_id : 8   action_name : foul\n",
      "action_id : 9   action_name : tackle\n",
      "action_id : 10   action_name : interception\n",
      "action_id : 11   action_name : shot\n",
      "action_id : 12   action_name : shot_penalty\n",
      "action_id : 13   action_name : shot_freekick\n",
      "action_id : 14   action_name : keeper_save\n",
      "action_id : 15   action_name : keeper_claim\n",
      "action_id : 16   action_name : keeper_punch\n",
      "action_id : 17   action_name : keeper_pick_up\n",
      "action_id : 18   action_name : clearance\n",
      "action_id : 19   action_name : bad_touch\n",
      "action_id : 20   action_name : non_action\n",
      "action_id : 21   action_name : dribble\n",
      "action_id : 22   action_name : goalkick\n"
     ]
    }
   ],
   "source": [
    "for idx, action_name in enumerate(actiontypes):\n",
    "    print(f'action_id : {idx}   action_name : {action_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodypart_id : 0   bodypart_name : foot\n",
      "bodypart_id : 1   bodypart_name : head\n",
      "bodypart_id : 2   bodypart_name : other\n",
      "bodypart_id : 3   bodypart_name : head/other\n",
      "bodypart_id : 4   bodypart_name : foot_left\n",
      "bodypart_id : 5   bodypart_name : foot_right\n"
     ]
    }
   ],
   "source": [
    "for idx, bodypart_name in enumerate(bodyparts):\n",
    "    print(f'bodypart_id : {idx}   bodypart_name : {bodypart_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_events_df_to_spadl(events_df, home_team_id):\n",
    "    spadl_events_df = convert_to_actions(events_df, home_team_id)\n",
    "    spadl_events_df['time_seconds'] = spadl_events_df['time_seconds'].astype('float64')\n",
    "    spadl_events_df['timestamp'] = pd.to_datetime(spadl_events_df['time_seconds'], unit='s')\n",
    "    spadl_events_df = spadl.play_left_to_right(spadl_events_df, home_team_id)\n",
    "    return spadl_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO ADD ADDITIONAL INFO IN RAW SPADL DATAFRAME\n",
    "STANDARD_LENGTH_COURT = 105\n",
    "STANDARD_WIDTH_COURT = 68\n",
    "STANDARD_GOALLINE_WIDTH = 7.32\n",
    "\n",
    "# Helper Functions\n",
    "def filter_out_is_home_team_apply_df(row, home_team_id):\n",
    "    return 1 if row['team_id'] == home_team_id else 0\n",
    "\n",
    "def filter_out_take_on_or_dribble_apply_df(row, take_on_action_id):\n",
    "    return 1 if row['action_id'] == take_on_action_id else 0\n",
    "\n",
    "# Add is_home_team column (boolean 0/1)\n",
    "def add_is_home_team_column_to_spadl_df(spadl_df, home_team_id):\n",
    "    spadl_df['is_home_team'] = spadl_df.apply(lambda x : filter_out_is_home_team_apply_df(x, home_team_id), axis=1)\n",
    "    return spadl_df\n",
    "\n",
    "# Add is_take_on column (boolean 0/1)\n",
    "def add_is_take_on_column_to_spadl_df(spadl_df, take_on_action_id):\n",
    "    spadl_df['is_take_on'] = spadl_df.apply(lambda x : filter_out_take_on_or_dribble_apply_df(x, take_on_action_id), axis=1)\n",
    "    return spadl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all dataset action specific type, export them to csv files\n",
    "# Take_on (action_id = 7), Dribble (action_id = 21)\n",
    "DRIBBLE_ACTION_ID = [7, 21] \n",
    "TAKE_ON_ACTION_ID = 7\n",
    "\n",
    "def collect_raw_dribble_spadl_df(source=\"Wyscout\", period=1):\n",
    "    api = api_wyscout\n",
    "    list_competitions_ids = []\n",
    "    list_game_ids = []\n",
    "\n",
    "    competitions_df = api.competitions()\n",
    "    for _, row in competitions_df.iterrows():\n",
    "        list_competitions_ids.append((row['competition_id'], row['season_id']))\n",
    "        \n",
    "    for competition_id, season_id in list_competitions_ids:\n",
    "        games_df = api.games(competition_id, season_id)\n",
    "        for _, row in games_df.iterrows():\n",
    "            list_game_ids.append((row['game_id'], row['home_team_id'], row['away_team_id']))\n",
    "            \n",
    "    for game_id, home_team_id, away_team_id in list_game_ids:\n",
    "        this_game_events_df = api.events(game_id)\n",
    "        this_game_events_spadl_df = convert_events_df_to_spadl(this_game_events_df, home_team_id)\n",
    "        this_game_events_spadl_df = this_game_events_spadl_df[this_game_events_spadl_df['type_id'].isin(DRIBBLE_ACTION_ID)]\n",
    "        if (period != None):\n",
    "            this_game_events_spadl_df = this_game_events_spadl_df[this_game_events_spadl_df['period_id'] == period]\n",
    "        else:\n",
    "            this_game_events_spadl_df = this_game_events_spadl_df[this_game_events_spadl_df['period_id'] == 1]\n",
    "        # Add additional computed column to support xDribble model\n",
    "        this_game_events_spadl_df = add_is_home_team_column_to_spadl_df(this_game_events_spadl_df, home_team_id)\n",
    "        this_game_events_spadl_df = add_is_take_on_column_to_spadl_df(this_game_events_spadl_df, TAKE_ON_ACTION_ID)\n",
    "\n",
    "        # Export to external csv iteratively\n",
    "        this_game_events_spadl_df.to_csv(f'data/training_data_xdribble_wyscout/{game_id}_{home_team_id}_{away_team_id}_xdribble_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS TO CREATE ALL DATASET PLAYERS\n",
    "def collect_raw_all_players_df(source=\"Wyscout\"):\n",
    "    api = api_wyscout\n",
    "    list_competitions_ids = []\n",
    "    list_game_ids = []\n",
    "\n",
    "    competitions_df = api.competitions()\n",
    "    for _, row in competitions_df.iterrows():\n",
    "        list_competitions_ids.append((row['competition_id'], row['season_id']))\n",
    "        \n",
    "    for competition_id, season_id in list_competitions_ids:\n",
    "        games_df = api.games(competition_id, season_id)\n",
    "        for _, row in games_df.iterrows():\n",
    "            list_game_ids.append((row['game_id'], row['home_team_id'], row['away_team_id']))\n",
    "\n",
    "    for game_id, home_team_id, away_team_id in list_game_ids:\n",
    "        players_df = api.players(game_id)\n",
    "        players_df.to_csv(f'data/training_data_players_wyscout/{game_id}_{home_team_id}_{away_team_id}_players_data.csv')\n",
    "\n",
    "def load_and_concat_players_df_from_csv(path_to_raw_players_df):\n",
    "    list_raw_players_df = []\n",
    "    for filename in os.listdir(path_to_raw_players_df):\n",
    "        f = os.path.join(path_to_raw_players_df, filename)\n",
    "        if os.path.isfile(f):\n",
    "            players_df = pd.read_csv(f)\n",
    "            list_raw_players_df.append(players_df)\n",
    "    merged_players_df = pd.concat(list_raw_players_df)\n",
    "    merged_players_df = merged_players_df.drop_duplicates(subset='player_id').reset_index()\n",
    "    return merged_players_df\n",
    "\n",
    "def load_csv_players_data_sofifa(path_to_sofifa_file):\n",
    "    return pd.read_csv(path_to_sofifa_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment it if players dataset already loaded\n",
    "# collect_raw_all_players_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge wyscout player datasets with sofifa datasets by matching string name\n",
    "def create_maps_for_name_matching_scores(list_unique_names_df_1, list_unique_names_df_2):\n",
    "    maps_name_matching_score = {}\n",
    "    for name_1 in list_unique_names_df_1:\n",
    "        for name_2 in list_unique_names_df_2:\n",
    "            maps_name_matching_score[(name_1, name_2)] = fuzz.ratio(name_1, name_2)\n",
    "    return maps_name_matching_score\n",
    "\n",
    "def filter_out_maps_for_name_matching_scores(maps_name_matching, threshold):\n",
    "    filtered_maps_name_matching = {}\n",
    "    for name_1, name_2 in maps_name_matching:\n",
    "        if (maps_name_matching[(name_1, name_2)] >= threshold):\n",
    "            filtered_maps_name_matching[(name_1, name_2)] = maps_name_matching[(name_1, name_2)]\n",
    "    return filtered_maps_name_matching\n",
    "\n",
    "def merge_big_dataframe_wyscout_with_sofifa(big_dataframe_players, sofifa_players_dataset, maps_name_matching_score):\n",
    "    # Preprocess both dataframes and add prefix 1- and 2- to all column names to avoid duplicate column names\n",
    "    big_dataframe_players.dropna(subset=['player_name'], inplace=True)\n",
    "    big_dataframe_players.rename(columns=lambda x: '1-'+x, inplace=True)\n",
    "    sofifa_players_dataset.dropna(subset=['full_name'], inplace=True)\n",
    "    sofifa_players_dataset.rename(columns=lambda x: '2-'+x, inplace=True)\n",
    "    # Merge into new empty dataframe one by one by iterating maps name matching score\n",
    "    big_dataframe_players_with_sofifa = pd.DataFrame(columns=list(big_dataframe_players.columns)+list(sofifa_players_dataset.columns), index=[0])\n",
    "    big_dataframe_players_with_sofifa.reset_index(inplace=True)\n",
    "    for name_1, name_2 in maps_name_matching_score:\n",
    "        row_from_big_dataframe_players = big_dataframe_players[big_dataframe_players['1-player_name'] == name_1].iloc[0]\n",
    "        row_from_sofifa_players_dataset = sofifa_players_dataset[sofifa_players_dataset['2-full_name'] == name_2].iloc[0]\n",
    "        new_row = pd.concat([row_from_big_dataframe_players, row_from_sofifa_players_dataset], axis=0, ignore_index=False)\n",
    "        new_row = pd.DataFrame([new_row]).reset_index()\n",
    "        big_dataframe_players_with_sofifa = pd.concat([big_dataframe_players_with_sofifa, new_row])\n",
    "    # Remove prefix 1- and 2- from final big datasets\n",
    "    big_dataframe_players_with_sofifa.rename(columns=lambda x: x[2:], inplace=True)\n",
    "    return big_dataframe_players_with_sofifa\n",
    "\n",
    "DIRECTORY_PLAYERS_CSV_DATAS = \"data/training_data_players_wyscout\"\n",
    "DIRECTORY_SOFIFA_CSV_DATAS = \"data/players_skill_dataset/sofifa_dataset_cleaned.csv\"\n",
    "DIRECTORY_WYSCOUT_CSV_DATAS = \"data/players_skill_dataset/wyscout_dataset_cleaned.csv\"\n",
    "DIRECTORY_FINAL_PLAYERS_CSV_DATAS = \"data/players_skill_dataset/final_players_skill_dataset.csv\"\n",
    "\n",
    "# COMMENT BELOW SNIPPET CODES IF FINAL PLAYER DATASETS WITH SKILL ALREADY GENERATED !!\n",
    "# big_dataframe_players = load_and_concat_players_df_from_csv(DIRECTORY_PLAYERS_CSV_DATAS)\n",
    "# big_dataframe_players.to_csv(DIRECTORY_WYSCOUT_CSV_DATAS)\n",
    "# sofifa_players_dataset = load_csv_players_data_sofifa(DIRECTORY_SOFIFA_CSV_DATAS)\n",
    "\n",
    "# maps_name_matching_score = create_maps_for_name_matching_scores(big_dataframe_players['player_name'].unique(), sofifa_players_dataset['full_name'].unique())\n",
    "# maps_name_matching_score = filter_out_maps_for_name_matching_scores(maps_name_matching_score, threshold=80)\n",
    "\n",
    "# big_dataframe_players_with_sofifa = merge_big_dataframe_wyscout_with_sofifa(big_dataframe_players, sofifa_players_dataset, maps_name_matching_score)\n",
    "# big_dataframe_players_with_sofifa.reset_index(inplace=True)\n",
    "# big_dataframe_players_with_sofifa = big_dataframe_players_with_sofifa.drop_duplicates(subset='player_id')\n",
    "# big_dataframe_players_with_sofifa.to_csv(DIRECTORY_FINAL_PLAYERS_CSV_DATAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DRIVER (comment it if csv files already loaded)\n",
    "# collect_raw_dribble_spadl_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv datas already retrieved then concat them into one big dataframe\n",
    "DIRECTORY_XDRIBBLE_CSV_DATAS = \"data/training_data_xdribble_wyscout\"\n",
    "\n",
    "def load_and_concat_xdribble_df_from_csv():\n",
    "    list_pass_event_df = []\n",
    "    for filename in os.listdir(DIRECTORY_XDRIBBLE_CSV_DATAS):\n",
    "        f = os.path.join(DIRECTORY_XDRIBBLE_CSV_DATAS, filename)\n",
    "        if os.path.isfile(f):\n",
    "            pass_event_df = pd.read_csv(f)\n",
    "            list_pass_event_df.append(pass_event_df)\n",
    "    return pd.concat(list_pass_event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>game_id_x</th>\n",
       "      <th>period_id</th>\n",
       "      <th>time_seconds</th>\n",
       "      <th>team_id_x</th>\n",
       "      <th>player_id</th>\n",
       "      <th>start_x</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_x</th>\n",
       "      <th>end_y</th>\n",
       "      <th>...</th>\n",
       "      <th>LWB</th>\n",
       "      <th>LDM</th>\n",
       "      <th>CDM</th>\n",
       "      <th>RDM</th>\n",
       "      <th>RWB</th>\n",
       "      <th>LB</th>\n",
       "      <th>LCB</th>\n",
       "      <th>CB</th>\n",
       "      <th>RCB</th>\n",
       "      <th>RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1694390</td>\n",
       "      <td>1</td>\n",
       "      <td>34.903349</td>\n",
       "      <td>11944</td>\n",
       "      <td>83574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.65</td>\n",
       "      <td>46.92</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>293</td>\n",
       "      <td>1694395</td>\n",
       "      <td>1</td>\n",
       "      <td>1176.814302</td>\n",
       "      <td>11944</td>\n",
       "      <td>83574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11.55</td>\n",
       "      <td>30.60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>407</td>\n",
       "      <td>2500701</td>\n",
       "      <td>1</td>\n",
       "      <td>2268.952514</td>\n",
       "      <td>3770</td>\n",
       "      <td>83574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.75</td>\n",
       "      <td>32.64</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>2500723</td>\n",
       "      <td>1</td>\n",
       "      <td>613.924669</td>\n",
       "      <td>3770</td>\n",
       "      <td>83574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.75</td>\n",
       "      <td>36.72</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>252</td>\n",
       "      <td>2500731</td>\n",
       "      <td>1</td>\n",
       "      <td>912.678306</td>\n",
       "      <td>3770</td>\n",
       "      <td>83574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.4</td>\n",
       "      <td>7.35</td>\n",
       "      <td>40.12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x  game_id_x  period_id  time_seconds  team_id_x  player_id  \\\n",
       "0             9    1694390          1     34.903349      11944      83574   \n",
       "1           293    1694395          1   1176.814302      11944      83574   \n",
       "2           407    2500701          1   2268.952514       3770      83574   \n",
       "3           194    2500723          1    613.924669       3770      83574   \n",
       "4           252    2500731          1    912.678306       3770      83574   \n",
       "\n",
       "   start_x  start_y  end_x  end_y  ...  LWB  LDM  CDM  RDM  RWB   LB  LCB  \\\n",
       "0      0.0     34.0  13.65  46.92  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1      0.0     34.0  11.55  30.60  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2      0.0     34.0  15.75  32.64  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "3      0.0     34.0  15.75  36.72  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "4      0.0     37.4   7.35  40.12  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "    CB  RCB   RB  \n",
       "0  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JOIN ALREADY CONSTRUCTED PLAYER SKILLS DATASET WITH ORIGIN EVENT DATASET WYSCOUT\n",
    "player_skills_dataset = pd.read_csv(DIRECTORY_FINAL_PLAYERS_CSV_DATAS)\n",
    "big_dataframe_xdribble_model = load_and_concat_xdribble_df_from_csv()\n",
    "big_dataframe_xdribble_model = big_dataframe_xdribble_model.merge(player_skills_dataset, how='inner',on='player_id')\n",
    "big_dataframe_xdribble_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_x</th>\n",
       "      <th>start_y</th>\n",
       "      <th>end_x</th>\n",
       "      <th>end_y</th>\n",
       "      <th>result_id</th>\n",
       "      <th>is_home_team</th>\n",
       "      <th>is_take_on</th>\n",
       "      <th>age</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kgs</th>\n",
       "      <th>...</th>\n",
       "      <th>long_shots</th>\n",
       "      <th>aggression</th>\n",
       "      <th>interceptions</th>\n",
       "      <th>positioning</th>\n",
       "      <th>vision</th>\n",
       "      <th>penalties</th>\n",
       "      <th>composure</th>\n",
       "      <th>marking</th>\n",
       "      <th>standing_tackle</th>\n",
       "      <th>sliding_tackle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.65</td>\n",
       "      <td>46.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>195.58</td>\n",
       "      <td>86.2</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11.55</td>\n",
       "      <td>30.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>195.58</td>\n",
       "      <td>86.2</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.75</td>\n",
       "      <td>32.64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>195.58</td>\n",
       "      <td>86.2</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>15.75</td>\n",
       "      <td>36.72</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>195.58</td>\n",
       "      <td>86.2</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>37.4</td>\n",
       "      <td>7.35</td>\n",
       "      <td>40.12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>195.58</td>\n",
       "      <td>86.2</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_x  start_y  end_x  end_y  result_id  is_home_team  is_take_on   age  \\\n",
       "0      0.0     34.0  13.65  46.92          1             0           0  33.0   \n",
       "1      0.0     34.0  11.55  30.60          1             1           0  33.0   \n",
       "2      0.0     34.0  15.75  32.64          1             1           0  33.0   \n",
       "3      0.0     34.0  15.75  36.72          1             1           0  33.0   \n",
       "4      0.0     37.4   7.35  40.12          1             0           0  33.0   \n",
       "\n",
       "   height_cm  weight_kgs  ...  long_shots  aggression  interceptions  \\\n",
       "0     195.58        86.2  ...        12.0        33.0           26.0   \n",
       "1     195.58        86.2  ...        12.0        33.0           26.0   \n",
       "2     195.58        86.2  ...        12.0        33.0           26.0   \n",
       "3     195.58        86.2  ...        12.0        33.0           26.0   \n",
       "4     195.58        86.2  ...        12.0        33.0           26.0   \n",
       "\n",
       "   positioning  vision  penalties  composure  marking  standing_tackle  \\\n",
       "0         13.0    48.0       16.0       45.0     12.0             15.0   \n",
       "1         13.0    48.0       16.0       45.0     12.0             15.0   \n",
       "2         13.0    48.0       16.0       45.0     12.0             15.0   \n",
       "3         13.0    48.0       16.0       45.0     12.0             15.0   \n",
       "4         13.0    48.0       16.0       45.0     12.0             15.0   \n",
       "\n",
       "   sliding_tackle  \n",
       "0            12.0  \n",
       "1            12.0  \n",
       "2            12.0  \n",
       "3            12.0  \n",
       "4            12.0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SELECT ONLY FEATURED COLUMN FROM BIG DATASETS\n",
    "features_column_included = [\"start_x\", \"start_y\", \"end_x\", \"end_y\", \"is_take_on\", \"is_home_team\", \"result_id\"]\n",
    "player_skills_column_included = [\"acceleration\", \"aggression\", \"agility\", \"balance\", \"ball_control\",\n",
    "                                 \"composure\", \"crossing\", \"curve\", \"dribbling\", \"finishing\",\n",
    "                                 \"freekick_accuracy\", \"heading_accuracy\", \"interceptions\", \"jumping\", \"long_passing\",\n",
    "                                 \"long_shots\", \"marking\", \"penalties\", \"positioning\", \"reactions\",\n",
    "                                 \"shot_power\", \"sliding_tackle\", \"sprint_speed\", \"stamina\", \"short_passing\",\n",
    "                                 \"standing_tackle\", \"strength\", \"vision\", \"volleys\"]\n",
    "player_attribute_column_included = [\"height_cm\", \"weight_kgs\", \"age\"]\n",
    "\n",
    "big_dataframe_xdribble_model = big_dataframe_xdribble_model[[c for c in big_dataframe_xdribble_model.columns if c in (features_column_included + player_skills_column_included + player_attribute_column_included)]]\n",
    "big_dataframe_xdribble_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 1 : Random Oversample Function\n",
    "def training_data_random_oversampled(X_train, Y_train):\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_resampled, Y_resampled = ros.fit_resample(X_train, Y_train)\n",
    "    return (X_resampled, Y_resampled)\n",
    "\n",
    "# CASE 2 : Random Undersample Function\n",
    "def training_data_random_undersampled(X_train, Y_train):\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    X_resampled, Y_resampled = rus.fit_resample(X_train, Y_train)\n",
    "    return (X_resampled, Y_resampled)\n",
    "\n",
    "# CASE 3 : Random SMOTE Oversample Function\n",
    "def training_data_smote_oversampled(X_train, Y_train):\n",
    "    X_resampled, Y_resampled = SMOTE().fit_resample(X_train, Y_train)\n",
    "    return (X_resampled, Y_resampled)\n",
    "\n",
    "# CASE 1 : Feature Selection - Pearson Coefficient\n",
    "def filter_columns_feature_selection_pearson(X_train, Y_train, columns_considered, threshold):\n",
    "    new_columns_after_selection = []\n",
    "    for _, skill in enumerate(columns_considered):\n",
    "        correlation_value, _ = pearsonr(X_train[skill], Y_train)\n",
    "        if correlation_value >= threshold:\n",
    "            new_columns_after_selection.append(skill)\n",
    "    return new_columns_after_selection\n",
    "\n",
    "def training_data_feature_selection_pearson(X_train, Y_train, columns_considered, threshold):\n",
    "    columns_filtered = filter_columns_feature_selection_pearson(X_train, Y_train, columns_considered, threshold)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)\n",
    "\n",
    "# CASE 2 : Feature Selection - Chi Square\n",
    "def filter_columns_feature_selection_chisquare(X_train, Y_train, columns_considered, num_of_features):\n",
    "    chi2_selector = SelectKBest(chi2, k=num_of_features) \n",
    "    df_feature = X_train[columns_considered]\n",
    "    chi2_selector.fit(df_feature, Y_train)\n",
    "    cols = chi2_selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_chisquare(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_filtered = filter_columns_feature_selection_chisquare(X_train, Y_train, columns_considered, num_of_features)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)\n",
    "\n",
    "# CASE 3 : Feature Selection - Mutual Information\n",
    "def filter_columns_feature_selection_mutualinf(X_train, Y_train, columns_considered, num_of_features):\n",
    "    mi_selector = SelectKBest(mutual_info_classif, k=num_of_features) \n",
    "    df_feature = X_train[columns_considered]\n",
    "    mi_selector.fit(df_feature, Y_train)\n",
    "    cols = mi_selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_mutualinf(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_filtered = filter_columns_feature_selection_mutualinf(X_train, Y_train, columns_considered, num_of_features)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)\n",
    "\n",
    "# CASE 4 : Feature Selection - mRMR Selection\n",
    "def filter_columns_feature_selection_mrmr(X_train, Y_train, columns_considered, num_of_features):\n",
    "    df_feature = X_train[columns_considered]\n",
    "    selected_features = mrmr_classif(X=df_feature, y=Y_train, K=num_of_features)\n",
    "    return selected_features\n",
    "\n",
    "def training_data_feature_selection_mrmr(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_filtered = filter_columns_feature_selection_mrmr(X_train, Y_train, columns_considered, num_of_features)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)\n",
    "\n",
    "# CASE 5 : Feature Selection - Sequential Forward Selection (SFS)\n",
    "def filter_columns_feature_selection_sfs(X_train, Y_train, columns_considered, num_of_features):\n",
    "    rf = RandomForestClassifier()\n",
    "    sfs = SequentialFeatureSelector(rf, n_features_to_select=num_of_features, direction='forward')\n",
    "    df_feature = X_train[columns_considered]\n",
    "    sfs.fit(df_feature, Y_train)\n",
    "    cols = sfs.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_sfs(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_filtered = filter_columns_feature_selection_sfs(X_train, Y_train, columns_considered, num_of_features)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)\n",
    "\n",
    "# CASE 6 : Feature Selection - Sequential Backward Elimination (SBE)\n",
    "def filter_columns_feature_selection_sbe(X_train, Y_train, columns_considered, num_of_features):\n",
    "    rf = RandomForestClassifier()\n",
    "    sfs = SequentialFeatureSelector(rf, n_features_to_select=num_of_features, direction='backward')\n",
    "    df_feature = X_train[columns_considered]\n",
    "    sfs.fit(df_feature, Y_train)\n",
    "    cols = sfs.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_sbe(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_filtered = filter_columns_feature_selection_sbe(X_train, Y_train, columns_considered, num_of_features)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)\n",
    "\n",
    "# CASE 7 : Feature Selection - Recursive Feature Elimination\n",
    "def filter_columns_feature_selection_rfe(X_train, Y_train, columns_considered, num_of_features):\n",
    "    estimator = LinearSVR()\n",
    "    selector = RFECV(estimator, step=1, cv=num_of_features)\n",
    "    df_feature = X_train[columns_considered]\n",
    "    selector.fit(df_feature, Y_train)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_rfe(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_filtered = filter_columns_feature_selection_rfe(X_train, Y_train, columns_considered, num_of_features)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)\n",
    "\n",
    "# CASE 8 : Feature Selection - Random Forest Embedded (rfembedded)\n",
    "def filter_columns_feature_selection_rfembedded(X_train, Y_train, columns_considered, num_of_features):\n",
    "    estimator = RandomForestClassifier()\n",
    "    selector = SelectFromModel(estimator=estimator, max_features=num_of_features)\n",
    "    df_feature = X_train[columns_considered]\n",
    "    selector.fit(df_feature, Y_train)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_rfembedded(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_filtered = filter_columns_feature_selection_rfembedded(X_train, Y_train, columns_considered, num_of_features)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)\n",
    "\n",
    "# CASE 9 : Feature Selection - LASSO\n",
    "def filter_columns_feature_selection_lasso(X_train, Y_train, columns_considered, num_of_features):\n",
    "    estimator = LogisticRegression(penalty='l2', C=0.5, solver='newton-cholesky')\n",
    "    selector = SelectFromModel(estimator=estimator, max_features=num_of_features)\n",
    "    df_feature = X_train[columns_considered]\n",
    "    selector.fit(df_feature, Y_train)\n",
    "    cols = selector.get_support(indices=True)\n",
    "    df_selected_features = df_feature.iloc[:,cols]\n",
    "    return df_selected_features.columns\n",
    "\n",
    "def training_data_feature_selection_lasso(X_train, Y_train, columns_considered, num_of_features):\n",
    "    columns_filtered = filter_columns_feature_selection_lasso(X_train, Y_train, columns_considered, num_of_features)\n",
    "    print(columns_filtered)\n",
    "    return (X_train[columns_filtered], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    99887\n",
      "0     7340\n",
      "Name: result_id, dtype: int64\n",
      "1.0    99887\n",
      "0.0     7340\n",
      "Name: result_id, dtype: int64\n",
      "Index(['ball_control', 'composure', 'dribbling', 'positioning',\n",
      "       'sliding_tackle', 'stamina', 'short_passing', 'standing_tackle'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# FEATURE PREPROCESSING BIG DATASETS AND CREATE XGBOOST MODEL\n",
    "# 1. Change all numeric columns with MinMaxScaler\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "columns_minmax_scaler = player_skills_column_included + player_attribute_column_included + [\"start_x\", \"start_y\", \"end_x\", \"end_y\"]\n",
    "big_dataframe_xdribble_model[columns_minmax_scaler] = scaler.fit_transform(big_dataframe_xdribble_model[columns_minmax_scaler])\n",
    "\n",
    "# 2. Check if data is unbalanced. If it is unbalanced, then do method to oversize the sample\n",
    "print(big_dataframe_xdribble_model['result_id'].value_counts())\n",
    "\n",
    "# 3. Change result_id label into float64 type\n",
    "big_dataframe_xdribble_model['result_id'] = big_dataframe_xdribble_model['result_id'].astype('float64')\n",
    "\n",
    "# 4. Remove dataframe instead of having result_id (0,1) --> (fail, success)\n",
    "big_dataframe_xdribble_model = big_dataframe_xdribble_model[big_dataframe_xdribble_model['result_id'].isin([0,1])]\n",
    "print(big_dataframe_xdribble_model['result_id'].value_counts())\n",
    "\n",
    "# 5. Split train data and test data from Big Datasets\n",
    "all_feature_columns = columns_minmax_scaler + [\"is_home_team\", \"is_take_on\"]\n",
    "X_train = big_dataframe_xdribble_model[all_feature_columns]\n",
    "Y_train = big_dataframe_xdribble_model[\"result_id\"]\n",
    "\n",
    "X_resampled, Y_resampled = SMOTE().fit_resample(X_train, Y_train)\n",
    "feature_selection_name = 'lasso'\n",
    "X, Y = globals()[\"training_data_feature_selection_\" + feature_selection_name](X_resampled, Y_resampled, player_skills_column_included, 10)\n",
    "\n",
    "# print(X_train.dtypes)\n",
    "# X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, Y_train, test_size=0.5)\n",
    "\n",
    "# # 6. Train XGBoost Model \n",
    "# modelXGB = XGBRegressor(objective=\"reg:logistic\")\n",
    "# modelXGB.fit(X_train_split, y_train_split)\n",
    "\n",
    "# # 7. Predict Testing Data\n",
    "# y_predict = modelXGB.predict(X_test_split)\n",
    "\n",
    "# # 8. Display report for prediction\n",
    "# print(\"Mean squared error : \", mean_squared_error(y_test_split, y_predict))\n",
    "\n",
    "# # 9. Save model to external file\n",
    "# filename = \"xgbRegressor_xdribble_model.sav\"\n",
    "# directory_model = \"data/model_xdribble_wyscout/\"\n",
    "# pickle.dump(modelXGB, open(directory_model + filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
